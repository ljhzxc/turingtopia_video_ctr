{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "import pickle\n",
    "path = 'D:\\\\ctr contest\\\\inter var\\\\before_feat_eng\\\\'\n",
    "\n",
    "file1 = open(path+'df.pkl','rb')\n",
    "df = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'click_df.pkl','rb')\n",
    "click_df = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'sort_df.pkl','rb')\n",
    "sort_df = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'labels.pkl','rb')\n",
    "labels = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'train_num.pkl','rb')\n",
    "train_num = pickle.load(file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "print('=============================================== feat eng ===============================================')\n",
    "\n",
    "print('*************************** embedding ***************************')\n",
    "# 之前有个朋友给embedding做了一个我认为非常形象的比喻：\n",
    "# 在非诚勿扰上面，如果你想了解一个女嘉宾，那么你可以看看她都中意过哪些男嘉宾；\n",
    "# 反过来也一样，如果你想认识一个男嘉宾，那么你也可以看看他都选过哪些女嘉宾。\n",
    "\n",
    "\n",
    "def emb(df, f1, f2):\n",
    "    emb_size = 8\n",
    "    print('====================================== {} {} ======================================'.format(f1, f2))\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg({'{}_{}_list'.format(f1, f2): list})\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, sg=0, hs=1, seed=2019)\n",
    "    emb_matrix = []\n",
    "   \n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "    emb_matrix = np.array(emb_matrix)   ##########3  diy\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}'.format(f1, f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    tmp = reduce_mem(tmp)\n",
    "    print('runtime:', time.time() - t)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "emb_cols = [\n",
    "    ['deviceid', 'newsid'],\n",
    "    ['deviceid', 'lng_lat'],\n",
    "    ['newsid', 'lng_lat'],\n",
    "    # ...\n",
    "]\n",
    "for f1, f2 in emb_cols:\n",
    "    df = df.merge(emb(sort_df, f1, f2), on=f1, how='left')\n",
    "    df = df.merge(emb(sort_df, f2, f1), on=f2, how='left')\n",
    "del sort_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('***************************deep walk embedding ***************************')\n",
    "import random\n",
    "def deepwalk(df, f1, f2):\n",
    "    L = 16\n",
    "    #Deepwalk算法，\n",
    "    print(\"deepwalk:\",f1,f2)\n",
    "    #构建图\n",
    "    dic={}\n",
    "    for item in df[[f1,f2]].values:\n",
    "        try:\n",
    "            str(int(item[1]))\n",
    "            str(int(item[0]))\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            dic['item_'+str(int(item[1]))].add('user_'+str(int(item[0])))\n",
    "        except:\n",
    "            dic['item_'+str(int(item[1]))]=set(['user_'+str(int(item[0]))])\n",
    "        try:\n",
    "            dic['user_'+str(int(item[0]))].add('item_'+str(int(item[1])))\n",
    "        except:\n",
    "            dic['user_'+str(int(item[0]))]=set(['item_'+str(int(item[1]))])\n",
    "    dic_cont={}\n",
    "    for key in dic:\n",
    "        dic[key]=list(dic[key])\n",
    "        dic_cont[key]=len(dic[key])\n",
    "    print(\"creating\")     \n",
    "    #构建路径\n",
    "    path_length=24\n",
    "    sentences=[]\n",
    "    length=[]\n",
    "    for key in dic:\n",
    "        sentence=[key]\n",
    "        while len(sentence)!=path_length:\n",
    "            key=dic[sentence[-1]][random.randint(0,dic_cont[sentence[-1]]-1)]\n",
    "            if len(sentence)>=2 and key == sentence[-2]:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(key)\n",
    "        sentences.append(sentence)\n",
    "        length.append(len(sentence))\n",
    "        if len(sentences)%100000==0:\n",
    "            print(len(sentences))\n",
    "    print(np.mean(length))\n",
    "    print(len(sentences))\n",
    "    #训练Deepwalk模型\n",
    "    print('training...')\n",
    "    random.shuffle(sentences)\n",
    "    model = Word2Vec(sentences, size=L, window=4,min_count=1,sg=1, workers=10,iter=20)\n",
    "    print('outputing...')\n",
    "    #输出\n",
    "    values=set(df[f1].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        try:\n",
    "            a=[int(v)]\n",
    "            a.extend(model['user_'+str(int(v))])\n",
    "            w2v.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    out_df1=pd.DataFrame(w2v)\n",
    "    names=[f1]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df1.columns = names\n",
    "    print(out_df1.head())\n",
    "    \n",
    "    ########################\n",
    "    values=set(df[f2].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        try:\n",
    "            a=[int(v)]\n",
    "            a.extend(model['item_'+str(int(v))])\n",
    "            w2v.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    out_df2=pd.DataFrame(w2v)\n",
    "    names=[f2]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_emb_'+str(L)+'_'+str(i))\n",
    "    out_df2.columns = names\n",
    "    print(out_df2.head())\n",
    "    return (out_df1, out_df2)\n",
    "\n",
    "\n",
    "emb_cols = [\n",
    "    ['deviceid', 'newsid'],\n",
    "#    ['lng_lat', 'newsid'],\n",
    "]\n",
    "for f1, f2 in emb_cols:\n",
    "    out_df1, out_df2 = deepwalk(sort_df, f1, f2)\n",
    "    df = df.merge(out_df1, on=f1, how='left')\n",
    "    del out_df1, out_df2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "user = pd.read_csv('E:\\\\contest\\\\ctr\\\\user.csv')\n",
    "countrow = 0\n",
    "userlist = []\n",
    "itemlist = []\n",
    "ratelist = []\n",
    "for item in user[user['tag']==user['tag']][['deviceid', 'tag']].values:\n",
    "    ta = item[1]\n",
    "    countrow +=1\n",
    "    if countrow% 10000==0:print(countrow)\n",
    "\n",
    "    try:\n",
    "        tasp = [[re.split(':|_',tt)[0], float(re.split(':|_',tt)[2])] for tt in  ta.split('|') if len(tt.split(':'))>1]\n",
    "\n",
    "        for tas in tasp:\n",
    "            userlist.append(item[0])\n",
    "            itemlist.append(tas[0])\n",
    "\n",
    "            ratelist.append(tas[1])\n",
    "    except:\n",
    "        print(ta)\n",
    "        \n",
    "train = pd.DataFrame(zip(userlist, itemlist, ratelist), columns=['userID', 'itemsID', 'rating'])\n",
    "train = train.sample(frac=1.0)\n",
    "train = train.reset_index(drop=True)\n",
    "train = train[train['rating']<4.57]\n",
    "\n",
    "import surprise\n",
    "\n",
    "train_set = surprise.Dataset.load_from_df(train, reader=surprise.Reader(rating_scale=(1, 4.5))).build_full_trainset()\n",
    "svd = surprise.SVD(random_state=0, n_factors=20, n_epochs=300, verbose=True, lr_all=0.0005)\n",
    "start_time = time.time()\n",
    "svd.fit(train_set)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "user_id_uni = train['userID'].unique()\n",
    "w2v=[]\n",
    "for us, svdfea in zip(user_id_uni, svd.pu):\n",
    "    a = [us]\n",
    "    a.extend(list(svdfea))\n",
    "    w2v.append(a)\n",
    "    \n",
    "out_df = pd.DataFrame(w2v, columns=['deviceid']+['tag_scd_'+str(i)+ '_feature' for i in range(20)])\n",
    "\n",
    "import pickle\n",
    "file1 = open('D:\\\\ctr contest\\\\inter var\\\\map_dict\\\\deviceid_map2.pkl', 'rb')\n",
    "did_map_dict = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "out_df['deviceid'] = out_df['deviceid'].apply(lambda x: did_map_dict[x])\n",
    "df = df.merge(out_df, on='deviceid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "out = 'D:\\\\ctr contest\\\\inter var\\\\features\\\\'\n",
    "file1 = open(out+'embedding_feature.pkl','wb')\n",
    "pickle.dump(df[df.columns.to_list()[26:]], file1, protocol = 4)\n",
    "file1.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
