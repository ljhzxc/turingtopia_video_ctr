{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "#         print(col)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "import pickle\n",
    "path = 'D:\\\\ctr contest\\\\inter var\\\\before_feat_eng\\\\'\n",
    "feat_path = 'D:\\\\ctr contest\\\\inter var\\\\features\\\\'\n",
    "\n",
    "file1 = open(path+'df.pkl','rb')\n",
    "df = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(feat_path+'history_stats_feature.pkl','rb')\n",
    "history_stats_feature = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(feat_path+'exposure_ts_gap_feature.pkl','rb')\n",
    "exposure_ts_gap_feature = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(feat_path+'cross_feature.pkl','rb')\n",
    "cross_feature = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(feat_path+'embedding_feature.pkl','rb')\n",
    "embedding_feature = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'train_num.pkl','rb')\n",
    "train_num = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file1 = open(path+'labels.pkl','rb')\n",
    "labels = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = pd.concat([df, history_stats_feature, exposure_ts_gap_feature, cross_feature, embedding_feature], axis=1)\n",
    "\n",
    "del df['ts']\n",
    "del df['id']\n",
    "del history_stats_feature\n",
    "del exposure_ts_gap_feature\n",
    "del cross_feature\n",
    "del embedding_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_feats = ['pos_newsid_next3_exposure_ts_gap', 'cross_pos_lng_lat_nunique', 'pos_deviceid_prev10_exposure_ts_gap', 'minute', 'pos_newsid_prev3_exposure_ts_gap', 'cross_pos_netmodel_nunique', 'cross_deviceid_newsid_nunique', 'newsid_next10_exposure_ts_gap', 'newsid_next3_exposure_ts_gap', 'pos_newsid_next10_exposure_ts_gap', 'lng_lat_count', 'cross_deviceid_newsid_count_ratio', 'pos_deviceid_prev_day_click_count', 'lat', 'cross_newsid_lng_lat_nunique', 'cross_newsid_netmodel_ent', 'cross_newsid_lng_lat_nunique_ratio_newsid_count', 'cross_newsid_pos_count_ratio', 'cross_newsid_deviceid_nunique', 'cross_newsid_pos_nunique', 'device_vendor', 'newsid_deviceid_emb_1', 'pos_newsid_next2_exposure_ts_gap', 'newsid_lng_lat_emb_6', 'cross_pos_newsid_nunique', 'cross_newsid_deviceid_ent', 'lng_lat_prev3_exposure_ts_gap', 'pos_lng_lat_next10_exposure_ts_gap', 'pos_deviceid_lng_lat_prev2_exposure_ts_gap', 'cross_deviceid_netmodel_nunique', 'pos_newsid_next5_exposure_ts_gap', 'pos_deviceid_lng_lat_prev5_exposure_ts_gap', 'pos_deviceid_lng_lat_prev10_exposure_ts_gap', 'deviceid_newsid_deviceid_deepwalk_embedding_16_14', 'lng_lat', 'cross_netmodel_lng_lat_nunique_ratio_netmodel_count', 'pos_deviceid_lng_lat_prev3_exposure_ts_gap', 'newsid_count', 'lng', 'cross_newsid_netmodel_nunique']\n",
    "tmp_feats = []\n",
    "for col in del_feats:\n",
    "    if col in df:\n",
    "        tmp_feats.append(col)\n",
    "df.drop(tmp_feats, axis=1, inplace=True)\n",
    "\n",
    "df = reduce_mem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "import random\n",
    "\n",
    "print('========================================================================================================')\n",
    "train_df = df[:train_num].reset_index(drop=True)\n",
    "test_df = df[train_num:].reset_index(drop=True)\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "train_idx = train_df[train_df['day'] < 10].index.tolist()\n",
    "val_idx = train_df[train_df['day'] == 10].index.tolist()\n",
    "\n",
    "train_x = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "train_y = labels[train_idx]\n",
    "val_x = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "val_y = labels[val_idx]\n",
    "\n",
    "del train_x['day'], val_x['day'], train_df['day'], test_df['day']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)\n",
    "print('========================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "print('=============================================== training validate ===============================================')\n",
    "clf = CatBoostClassifier(iterations=25000,\n",
    "                           learning_rate=0.08,\n",
    "                           eval_metric='AUC',\n",
    "                           use_best_model=True,\n",
    "                           random_seed=42,\n",
    "                           task_type='GPU',\n",
    "                           devices='0:1',\n",
    "                           early_stopping_rounds=500,\n",
    "                           loss_function='Logloss',\n",
    "                           depth=7,\n",
    "                           verbose=100, \n",
    "                           )\n",
    "\n",
    "dtrain = Pool(data=train_x, label=train_y)\n",
    "dval =  Pool(data=val_x, label=val_y)\n",
    "\n",
    "clf.fit(\n",
    "    dtrain,\n",
    "    eval_set=dval,\n",
    ")\n",
    "print('runtime:', time.time() - t)\n",
    "\n",
    "best_rounds = clf.best_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf.predict_proba(val_x)[:, 1]\n",
    "print('=============================================== threshold search ===============================================')\n",
    "# f1阈值敏感，所以对阈值做一个简单的迭代搜索\n",
    "t0 = 0.05\n",
    "v = 0.002\n",
    "best_t = t0\n",
    "best_f1 = 0\n",
    "\n",
    "#为了快一点，缩小范围\n",
    "for step in range(150,191):\n",
    "#for step in range(140,250,3):\n",
    "    curr_t = t0 + step * v\n",
    "    y = [1 if x >= curr_t else 0 for x in val_pred]\n",
    "    curr_f1 = f1_score(val_y, y)\n",
    "    \n",
    "    if curr_f1 > best_f1:\n",
    "        best_t = curr_t\n",
    "        best_f1 = curr_f1\n",
    "        print('step: {}   best threshold: {}   best f1: {}'.format(step, best_t, best_f1))\n",
    "print('search finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== training predict ===============================================')\n",
    "best_rounds = 13677\n",
    "clf = CatBoostClassifier(iterations=best_rounds,\n",
    "                           learning_rate=0.08,\n",
    "                           eval_metric='AUC',\n",
    "                           use_best_model=True,\n",
    "                           random_seed=42,\n",
    "                           task_type='GPU',\n",
    "                           devices='0:1',\n",
    "                           #early_stopping_rounds=100,\n",
    "                           loss_function='Logloss',\n",
    "                           depth=7,\n",
    "                           verbose=100, \n",
    "                           )\n",
    "\n",
    "dtrain = Pool(data=train_df, label=labels)\n",
    "\n",
    "\n",
    "print('************** training **************')\n",
    "clf.fit(\n",
    "    dtrain\n",
    "    eval_set=[(train_d, labels)],\n",
    ")\n",
    "print('runtime:', time.time() - t)\n",
    "target = clf.predict_proba(test_df)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('D:\\\\ctr contest\\\\ctr\\\\sample.csv')\n",
    "sub['target'] = target\n",
    "best_t = 0.388\n",
    "sub.to_csv('D:\\\\ctr contest\\\\'+'sub_cat_3_prob_{}.csv'.format( sub['target'].mean()), index=False)\n",
    "sub['target'] = sub['target'].apply(lambda x: 1 if x >= best_t else 0)\n",
    "sub.to_csv('D:\\\\ctr contest\\\\'+'sub_cat_3_{}.csv'.format( sub['target'].mean()), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
